#!/usr/bin/env python3
"""
===============================================================================
 TP Log Analyzer - CSV ë°ì´í„° ìë™ ë¶„ì„ ë° ì´ìƒ ê°ì§€
===============================================================================
 ì‚¬ìš©ë²•:
   python3 cursor_analyze.py --input result.csv --threshold cursor_threshold.json

 ì¶œë ¥:
   - ì½˜ì†”ì— ë¶„ì„ ê²°ê³¼ ì¶œë ¥
   - cursor_analysis_report.txt íŒŒì¼ ìƒì„±
   - cursor_analysis_summary.json íŒŒì¼ ìƒì„± (ì‹œê°í™” ì—°ë™ìš©)
===============================================================================
"""

import argparse
import csv
import json
import statistics
from typing import List, Dict, Any
from datetime import datetime


def load_csv(path: str) -> List[Dict[str, Any]]:
    """CSV íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # ìˆ«ì ë³€í™˜
            converted = {}
            for k, v in row.items():
                try:
                    if '.' in str(v):
                        converted[k] = float(v)
                    else:
                        converted[k] = int(v)
                except (ValueError, TypeError):
                    converted[k] = v
            data.append(converted)
    return data


def load_threshold(path: str) -> Dict:
    """ì„ê³„ê°’ ì„¤ì • ë¡œë“œ"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©
        return get_default_thresholds()


def get_default_thresholds() -> Dict:
    """ê¸°ë³¸ ì„ê³„ê°’ (ë‚´ë¶€ì—ì„œ ìˆ˜ì • ê°€ëŠ¥)"""
    return {
        "gc_tokens": {
            "critical_min": 3,
            "warning_min": 5,
            "description": "GC Tokenì´ ì´ ê°’ ì´í•˜ë©´ ê²½ê³ "
        },
        "int_write_buf": {
            "critical_max": 58,
            "warning_max": 50,
            "description": "Internal Write Bufferê°€ ì´ ê°’ ì´ìƒì´ë©´ ê²½ê³ "
        },
        "fcore_runtime_us": {
            "critical_max": 900,
            "warning_max": 700,
            "description": "FCore ëŸ°íƒ€ì„ì´ ì´ ê°’ ì´ìƒì´ë©´ ê²½ê³ "
        },
        "qcc_count": {
            "spike_threshold_sigma": 2.0,
            "description": "í‰ê·  ëŒ€ë¹„ N ì‹œê·¸ë§ˆ ì´ìƒì´ë©´ ìŠ¤íŒŒì´í¬ë¡œ íŒë‹¨"
        }
    }


def calculate_stats(data: List[Dict], field: str) -> Dict:
    """íŠ¹ì • í•„ë“œì˜ í†µê³„ ê³„ì‚°"""
    values = [row.get(field, 0) for row in data if row.get(field) is not None]
    
    if not values:
        return {"error": "No data"}
    
    return {
        "count": len(values),
        "min": min(values),
        "max": max(values),
        "avg": round(statistics.mean(values), 2),
        "median": round(statistics.median(values), 2),
        "stdev": round(statistics.stdev(values), 2) if len(values) > 1 else 0,
        "p95": round(sorted(values)[int(len(values) * 0.95)], 2),
        "p99": round(sorted(values)[int(len(values) * 0.99)], 2),
    }


def detect_anomalies(data: List[Dict], thresholds: Dict) -> List[Dict]:
    """ì´ìƒ íŒ¨í„´ ê°ì§€"""
    anomalies = []
    
    # í•„ë“œë³„ í†µê³„ ê³„ì‚°
    stats_cache = {}
    for field in thresholds.keys():
        if any(field in row for row in data):
            stats_cache[field] = calculate_stats(data, field)
    
    for i, row in enumerate(data):
        record_anomalies = []
        
        # GC Token ê³ ê°ˆ ì²´í¬
        if 'gc_tokens' in row and 'gc_tokens' in thresholds:
            th = thresholds['gc_tokens']
            val = row['gc_tokens']
            if val <= th.get('critical_min', 3):
                record_anomalies.append({
                    "field": "gc_tokens",
                    "severity": "CRITICAL",
                    "value": val,
                    "threshold": th.get('critical_min'),
                    "message": f"GC Token ê³ ê°ˆ ìœ„í—˜! ({val} <= {th.get('critical_min')})"
                })
            elif val <= th.get('warning_min', 5):
                record_anomalies.append({
                    "field": "gc_tokens",
                    "severity": "WARNING",
                    "value": val,
                    "threshold": th.get('warning_min'),
                    "message": f"GC Token ë¶€ì¡± ({val} <= {th.get('warning_min')})"
                })
        
        # Buffer í¬í™” ì²´í¬
        if 'int_write_buf' in row and 'int_write_buf' in thresholds:
            th = thresholds['int_write_buf']
            val = row['int_write_buf']
            if val >= th.get('critical_max', 58):
                record_anomalies.append({
                    "field": "int_write_buf",
                    "severity": "CRITICAL",
                    "value": val,
                    "threshold": th.get('critical_max'),
                    "message": f"Write Buffer í¬í™”! ({val} >= {th.get('critical_max')})"
                })
            elif val >= th.get('warning_max', 50):
                record_anomalies.append({
                    "field": "int_write_buf",
                    "severity": "WARNING",
                    "value": val,
                    "threshold": th.get('warning_max'),
                    "message": f"Write Buffer ë†’ìŒ ({val} >= {th.get('warning_max')})"
                })
        
        # FCore ëŸ°íƒ€ì„ ìŠ¤íŒŒì´í¬ ì²´í¬
        if 'fcore_runtime_us' in row and 'fcore_runtime_us' in thresholds:
            th = thresholds['fcore_runtime_us']
            val = row['fcore_runtime_us']
            if val >= th.get('critical_max', 900):
                record_anomalies.append({
                    "field": "fcore_runtime_us",
                    "severity": "CRITICAL",
                    "value": val,
                    "threshold": th.get('critical_max'),
                    "message": f"FCore ëŸ°íƒ€ì„ ê¸‰ì¦! ({val}us >= {th.get('critical_max')}us)"
                })
        
        # QCC ìŠ¤íŒŒì´í¬ ì²´í¬ (ì‹œê·¸ë§ˆ ê¸°ë°˜)
        if 'qcc_count' in row and 'qcc_count' in stats_cache:
            stats = stats_cache['qcc_count']
            th = thresholds.get('qcc_count', {})
            sigma = th.get('spike_threshold_sigma', 2.0)
            
            val = row['qcc_count']
            threshold_val = stats['avg'] + (sigma * stats['stdev'])
            
            if val > threshold_val:
                record_anomalies.append({
                    "field": "qcc_count",
                    "severity": "WARNING",
                    "value": val,
                    "threshold": round(threshold_val, 2),
                    "message": f"QCC íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ({val} > {sigma}Ïƒ = {round(threshold_val, 2)})"
                })
        
        if record_anomalies:
            anomalies.append({
                "record_index": i,
                "timestamp": row.get('timestamp_ms', 'N/A'),
                "issues": record_anomalies
            })
    
    return anomalies


def generate_report(data: List[Dict], anomalies: List[Dict], thresholds: Dict) -> str:
    """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = []
    report.append("=" * 70)
    report.append(" TP LOG ANALYSIS REPORT")
    report.append(f" Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("=" * 70)
    report.append("")
    
    # 1. ê¸°ë³¸ í†µê³„
    report.append("[ BASIC STATISTICS ]")
    report.append("-" * 50)
    
    numeric_fields = [k for k in data[0].keys() if isinstance(data[0].get(k), (int, float)) and not k.startswith('_')]
    
    for field in numeric_fields[:10]:  # ìƒìœ„ 10ê°œë§Œ
        stats = calculate_stats(data, field)
        if 'error' not in stats:
            report.append(f"  {field}:")
            report.append(f"    Min: {stats['min']}, Max: {stats['max']}, Avg: {stats['avg']}")
            report.append(f"    P95: {stats['p95']}, P99: {stats['p99']}, StDev: {stats['stdev']}")
            report.append("")
    
    # 2. ì´ìƒ ì§•í›„ ìš”ì•½
    report.append("")
    report.append("[ ANOMALY SUMMARY ]")
    report.append("-" * 50)
    
    critical_count = sum(1 for a in anomalies for i in a['issues'] if i['severity'] == 'CRITICAL')
    warning_count = sum(1 for a in anomalies for i in a['issues'] if i['severity'] == 'WARNING')
    
    report.append(f"  ğŸ”´ CRITICAL: {critical_count} events")
    report.append(f"  ğŸŸ¡ WARNING:  {warning_count} events")
    report.append(f"  Total anomalous records: {len(anomalies)} / {len(data)}")
    report.append("")
    
    # 3. ì´ìƒ ìƒì„¸
    if anomalies:
        report.append("")
        report.append("[ ANOMALY DETAILS (Top 20) ]")
        report.append("-" * 50)
        
        for anomaly in anomalies[:20]:
            report.append(f"  Record #{anomaly['record_index']} (ts: {anomaly['timestamp']})")
            for issue in anomaly['issues']:
                icon = "ğŸ”´" if issue['severity'] == 'CRITICAL' else "ğŸŸ¡"
                report.append(f"    {icon} {issue['message']}")
            report.append("")
    
    # 4. ê¶Œì¥ ì‚¬í•­
    report.append("")
    report.append("[ RECOMMENDATIONS ]")
    report.append("-" * 50)
    
    if critical_count > 0:
        report.append("  âš ï¸  Critical ì´ìŠˆ ë°œê²¬ë¨. ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”:")
        
        # ì´ìŠˆ ìœ í˜•ë³„ ê¶Œì¥ì‚¬í•­
        issue_types = set(i['field'] for a in anomalies for i in a['issues'])
        
        if 'gc_tokens' in issue_types:
            report.append("    - GC Token ê³ ê°ˆ: GC threshold ì¡°ì • ë˜ëŠ” OP ë¹„ìœ¨ ì¦ê°€ ê²€í† ")
        if 'int_write_buf' in issue_types:
            report.append("    - Buffer í¬í™”: Buffer pool í¬ê¸° í™•ì¥ ë˜ëŠ” Host ì“°ë¡œí‹€ë§ ê²€í† ")
        if 'fcore_runtime_us' in issue_types:
            report.append("    - FCore ëŸ°íƒ€ì„ ê¸‰ì¦: Task ìŠ¤ì¼€ì¤„ë§ ìµœì í™” í•„ìš”")
    else:
        report.append("  âœ… Critical ì´ìŠˆ ì—†ìŒ. ì •ìƒ ë²”ìœ„ ë‚´ ë™ì‘ ì¤‘.")
    
    report.append("")
    report.append("=" * 70)
    
    return "\n".join(report)


def generate_summary_json(data: List[Dict], anomalies: List[Dict]) -> Dict:
    """ì‹œê°í™” ì—°ë™ìš© JSON ìš”ì•½"""
    numeric_fields = [k for k in data[0].keys() if isinstance(data[0].get(k), (int, float)) and not k.startswith('_')]
    
    stats_summary = {}
    for field in numeric_fields:
        stats_summary[field] = calculate_stats(data, field)
    
    return {
        "generated_at": datetime.now().isoformat(),
        "total_records": len(data),
        "anomaly_count": len(anomalies),
        "critical_count": sum(1 for a in anomalies for i in a['issues'] if i['severity'] == 'CRITICAL'),
        "warning_count": sum(1 for a in anomalies for i in a['issues'] if i['severity'] == 'WARNING'),
        "statistics": stats_summary,
        "anomalies": anomalies[:100],  # ìƒìœ„ 100ê°œë§Œ
    }


def main():
    parser = argparse.ArgumentParser(
        description='TP Log Analyzer - CSV ë°ì´í„° ìë™ ë¶„ì„',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--input', required=True, help='ë¶„ì„í•  CSV íŒŒì¼')
    parser.add_argument('--threshold', help='ì„ê³„ê°’ ì„¤ì • JSON (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)')
    parser.add_argument('--output-report', default='cursor_analysis_report.txt', help='ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--output-json', default='cursor_analysis_summary.json', help='JSON ìš”ì•½ ì¶œë ¥ íŒŒì¼')
    
    args = parser.parse_args()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"[INFO] Loading CSV: {args.input}")
    data = load_csv(args.input)
    print(f"  - Loaded {len(data)} records")
    
    # 2. ì„ê³„ê°’ ë¡œë“œ
    if args.threshold:
        print(f"[INFO] Loading thresholds: {args.threshold}")
        thresholds = load_threshold(args.threshold)
    else:
        print("[INFO] Using default thresholds")
        thresholds = get_default_thresholds()
    
    # 3. ì´ìƒ ê°ì§€
    print("[INFO] Detecting anomalies...")
    anomalies = detect_anomalies(data, thresholds)
    print(f"  - Found {len(anomalies)} anomalous records")
    
    # 4. ë¦¬í¬íŠ¸ ìƒì„±
    report = generate_report(data, anomalies, thresholds)
    print("\n" + report)
    
    # 5. íŒŒì¼ ì €ì¥
    with open(args.output_report, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\n[INFO] Report saved: {args.output_report}")
    
    summary = generate_summary_json(data, anomalies)
    with open(args.output_json, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"[INFO] JSON summary saved: {args.output_json}")


if __name__ == '__main__':
    main()

